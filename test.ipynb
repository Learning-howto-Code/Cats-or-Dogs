{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Dataset Composition:\n",
      "Clean 5 gpm: 4366 images\n",
      "Clean 2.5 gpm: 1829 images\n",
      "Clean .75 gpm: 1123 images\n",
      "Clean 1.75 gpm: 1810 images\n",
      "\n",
      "Target images per class: 1123\n",
      "Removed 3243 images from Clean 5 gpm\n",
      "Removed 706 images from Clean 2.5 gpm\n",
      "Removed 687 images from Clean 1.75 gpm\n",
      "\n",
      "Final Dataset Composition:\n",
      "Clean 5 gpm: 1123 images\n",
      "Clean 2.5 gpm: 1123 images\n",
      "Clean .75 gpm: 1123 images\n",
      "Clean 1.75 gpm: 1123 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def balance_image_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Balances the number of images across all class directories in a dataset.\n",
    "    \n",
    "    This function is designed to address class imbalance in image classification datasets\n",
    "    by ensuring each class has an equal number of images, which is crucial for training\n",
    "    fair and accurate Convolutional Neural Networks (CNNs).\n",
    "    \n",
    "    Key Steps:\n",
    "    1. Identify all class directories\n",
    "    2. Count the number of images in each class\n",
    "    3. Find the minimum number of images across all classes\n",
    "    4. Randomly remove excess images from classes with more images\n",
    "    \n",
    "    Args:\n",
    "    dataset_path (str): Full path to the root directory containing class subdirectories\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing the number of images in each class after balancing\n",
    "    \"\"\"\n",
    "    # Step 1: Discover Class Directories\n",
    "    # We assume each subdirectory represents a unique class for the CNN\n",
    "    class_dirs = [\n",
    "        d for d in os.listdir(dataset_path) \n",
    "        if os.path.isdir(os.path.join(dataset_path, d))\n",
    "    ]\n",
    "    \n",
    "    # Step 2: Count Images in Each Class Directory\n",
    "    # Support a wide range of image file extensions to ensure comprehensive detection\n",
    "    image_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp']\n",
    "    \n",
    "    # Create a dictionary to store the number of images in each class\n",
    "    image_counts = {}\n",
    "    for class_dir in class_dirs:\n",
    "        full_path = os.path.join(dataset_path, class_dir)\n",
    "        \n",
    "        # Carefully count image files, excluding hidden or non-image files\n",
    "        image_files = [\n",
    "            f for f in os.listdir(full_path) \n",
    "            if any(f.lower().endswith(ext) for ext in image_extensions)\n",
    "        ]\n",
    "        \n",
    "        image_counts[class_dir] = len(image_files)\n",
    "    \n",
    "    # Step 3: Determine the Minimum Number of Images\n",
    "    # This becomes our target number of images for each class\n",
    "    min_images = min(image_counts.values())\n",
    "    \n",
    "    # Provide initial dataset information\n",
    "    print(\"Initial Dataset Composition:\")\n",
    "    for class_dir, count in image_counts.items():\n",
    "        print(f\"{class_dir}: {count} images\")\n",
    "    print(f\"\\nTarget images per class: {min_images}\")\n",
    "    \n",
    "    # Step 4: Balance the Dataset\n",
    "    balanced_counts = {}\n",
    "    for class_dir, count in image_counts.items():\n",
    "        full_path = os.path.join(dataset_path, class_dir)\n",
    "        \n",
    "        # If this class has more images than the minimum, remove excess\n",
    "        if count > min_images:\n",
    "            # Gather all image files in the directory\n",
    "            image_files = [\n",
    "                f for f in os.listdir(full_path) \n",
    "                if any(f.lower().endswith(ext) for ext in image_extensions)\n",
    "            ]\n",
    "            \n",
    "            # Randomly select files to remove\n",
    "            # This ensures a random sampling of images to remove\n",
    "            excess_files = random.sample(image_files, count - min_images)\n",
    "            \n",
    "            # Remove excess files\n",
    "            for file in excess_files:\n",
    "                file_path = os.path.join(full_path, file)\n",
    "                os.remove(file_path)\n",
    "            \n",
    "            print(f\"Removed {count - min_images} images from {class_dir}\")\n",
    "        \n",
    "        # Record the final count for this class\n",
    "        balanced_counts[class_dir] = min(count, min_images)\n",
    "    \n",
    "    # Final Report\n",
    "    print(\"\\nFinal Dataset Composition:\")\n",
    "    for class_dir, count in balanced_counts.items():\n",
    "        print(f\"{class_dir}: {count} images\")\n",
    "    \n",
    "    return balanced_counts\n",
    "\n",
    "# Practical Usage Example\n",
    "# Make sure to replace with your actual dataset path\n",
    "dataset_path = '/Users/jakehopkins/Desktop/clean:dirty/Train'\n",
    "final_image_counts = balance_image_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "# Assuming you have already trained your model and have validation data\n",
    "# Predict the labels for the validation dataset\n",
    "val_labels = np.concatenate([y for x, y in validation_dataset], axis=0)\n",
    "val_predictions = model.predict(validation_dataset)\n",
    "val_predictions = np.round(val_predictions).astype(int).flatten()\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_predictions)\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Cat', 'Dog'], yticklabels=['Cat', 'Dog'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
